# Ollama Chat + CrewAI Integration ğŸš€

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.9+-blue?logo=python" alt="Python">
  <img src="https://img.shields.io/badge/Framework-Streamlit-red?logo=streamlit" alt="Streamlit">
  <img src="https://img.shields.io/badge/LLM-Ollama-FFD43B?logo=ollama" alt="Ollama">
  <img src="https://img.shields.io/badge/AI-CrewAI-6DB33F" alt="CrewAI">
</div>

## ğŸ“ DescriÃ§Ã£o
AplicaÃ§Ã£o web que integra **CrewAI** com modelos LLM locais via **Ollama**, utilizando Streamlit para interface. Permite interaÃ§Ã£o com modelos como LLaMA3 diretamente do seu hardware.

## âœ¨ Funcionalidades
- âœ… Chat com LLM local (sem dependÃªncia de APIs externas)
- âœ… Arquitetura modular com agentes/tarefas do CrewAI
- âœ… Interface simples via Streamlit
- âœ… Suporte a mÃºltiplos modelos do Ollama

## ğŸ› ï¸ PrÃ©-requisitos
- Python 3.9+
- Ollama instalado ([Guia de instalaÃ§Ã£o](https://ollama.ai/))
- Modelo LLaMA3 baixado (ou outro suportado)

## ğŸš€ Como Executar
```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/ollama-crewai-chat.git
cd ollama-crewai-chat

# 2. Instale as dependÃªncias
pip install -r requirements.txt

# 3. Baixe o modelo (se nÃ£o tiver)
ollama pull llama3:3b

# 4. Execute a aplicaÃ§Ã£o
streamlit run app.py
